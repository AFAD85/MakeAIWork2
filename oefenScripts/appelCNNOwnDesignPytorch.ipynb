{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zelfde model als op Colab (appelCNNOwnDesign.ipynb, maar iets verder uitgewerkt)\n",
    "\n",
    "Maar dan in Torch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 63\u001b[0m\n\u001b[0;32m     50\u001b[0m         sample \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:image, \u001b[39m#preprocessed image, for input into NN\u001b[39;00m\n\u001b[0;32m     51\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m:label,\n\u001b[0;32m     52\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mimg_idx\u001b[39m\u001b[39m'\u001b[39m:idx}\n\u001b[0;32m     53\u001b[0m         \u001b[39mreturn\u001b[39;00m \n\u001b[1;32m---> 63\u001b[0m trainset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset(\u001b[39m'\u001b[39;49m\u001b[39m../projects/apple_disease_classification/data\u001b[39;49m\u001b[39m'\u001b[39;49m, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     64\u001b[0m                                         transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[0;32m     65\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(trainset, batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     66\u001b[0m                                           shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     68\u001b[0m testset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../projects/apple_disease_classification/data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m                                        transform\u001b[39m=\u001b[39mtransform)\n",
      "\u001b[1;31mTypeError\u001b[0m: Dataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "#data voorbewerken (pixels tussen 0 en 1 plaatsen door de waardes door 255 te delen, de ToTensor klasse(?) doet dit\n",
    "batch_size = 55\n",
    "# Batch size in paper was 55, dus mocht ik ook 1000 fotos per klasse hebben dan staat deze al goed (anders terug op 32 zetten)\n",
    "img_height = 350\n",
    "img_width = 350\n",
    "\n",
    "# voor meer transformaties in 1 keer kan je transforms.Compose([transforms.blah,transforms.blah2]) gebruiken\n",
    "transform = transforms.ToTensor()\n",
    "#transforms.RandomPosterize\n",
    "\n",
    "\n",
    "\n",
    "class AppelsTrainData(Dataset):\n",
    "    # hier gaan we de dataset definieren\n",
    "    def __init__(self, setname):\n",
    "        self.setname = setname\n",
    "        # setname bepaalt of we de trainings (train) validatie (val), of test set hebben.\n",
    "        # assert (nog nooit hiervoor gebruikt) checkt of de setname 1 van de 3 in de lijst is, zo ja gaat ie door anders : assertion error\n",
    "        assert setname in ['train','val','test'],'Please enter one of the follow as setname: train, val, test'\n",
    "        \n",
    "        overall_train_dataset_dir = 'projects/apple_disease_classification/data'\n",
    "        self.selected_dataset_dir = overall_train_dataset_dir,setname\n",
    "        \n",
    "                #E.g. self.all_filenames = ['006.png','007.png','008.png'] when setname=='val'\n",
    "        self.all_filenames = os.listdir(self.selected_dataset_dir)\n",
    "        self.all_labels = pd.read_csv(os.path.join(overall_dataset_dir,'tiny_labels.csv'),header=0,index_col=0)\n",
    "        self.label_meanings = self.all_labels.columns.values.tolist()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of examples in this split, e.g. if\n",
    "        self.setname=='train' then return the total number of examples\n",
    "        in the training set\"\"\"\n",
    "        return len(self.all_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"Return the example at index [idx]. The example is a dict with keys\n",
    "        'data' (value: Tensor for an RGB image) and 'label' (value: multi-hot\n",
    "        vector as Torch tensor of gr truth class labels).\"\"\"\n",
    "        selected_filename = self.all_filenames[idx]\n",
    "        imagepil = PIL.Image.open(os.path.join(self.selected_dataset_dir,selected_filename)).convert('RGB')\n",
    "        \n",
    "        #convert image to Tensor and normalize\n",
    "        image = torch.utils.to_tensor_and_normalize(imagepil)\n",
    "        \n",
    "        #load label\n",
    "        label = torch.Tensor(self.all_labels.loc[selected_filename,:].values)\n",
    "        \n",
    "        sample = {'data':image, #preprocessed image, for input into NN\n",
    "                  'label':label,\n",
    "                  'img_idx':idx}\n",
    "        return \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.Dataset('../projects/apple_disease_classification/data', train=True,\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torch.utils.data.Dataset(root='../projects/apple_disease_classification/data', train=False,\n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "# alternatief voor de loader:\n",
    "# train_data_loader = torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True, num_workers=16) \n",
    "classes = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Alternatieve versie, ff beide proberen en kijken wat lekkerder werkt, goede OOP oefening lijkt me zwz\n",
    "    Deze komt van pytorch.org (tutorials)\n",
    "    \"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \n",
    "    # The __init__ function is run once when instantiating the Dataset object. \n",
    "    # We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
    "        def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    # gewoon het aantal items in je set\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx. \n",
    "    # Based on the index, it identifies the imageâ€™s location on disk, converts that to a tensor using read_image, \n",
    "    # retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), \n",
    "    # and returns the tensor image and corresponding label in a tuple.\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 3\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(training_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppelClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5,self).__init__()\n",
    "\n",
    "        self.c1 = nn.Conv2d(in_channels=1,out_channels=128,kernel_size=3,stride=1,padding=0)\n",
    "        self.c2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5,stride=1,padding=0)\n",
    "        self.c3 = nn.Conv2d(in_channels=16,out_channels=120,kernel_size=5,stride=1,padding=0)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=120,out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84,out_features=10)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.c1(img)\n",
    "        x = self.relu(self.max_pool(x))\n",
    "        x = self.c2(x)\n",
    "        x = self.relu(self.max_pool(x))\n",
    "        x = self.relu(self.c3(x))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
