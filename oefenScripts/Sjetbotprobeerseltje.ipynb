{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# #Our sentences we like to encode\n",
    "# sentences = ['Help ik zit in de computer!',\n",
    "#     'Appeltjes van Oranje!',\n",
    "#     'Zeg die vragen van u, vinde gij daar zelf van?']\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# embeddings = model.encode(sentences)\n",
    "\n",
    "# #Print the embeddings\n",
    "# for sentence, embedding in zip(sentences, embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# emb1 = model.encode(\"Geen moeilijke vragen stelen he\")\n",
    "# emb2 = model.encode(\"Ik wordt daar heel appelig van\")\n",
    "\n",
    "# cos_sim = util.cos_sim(emb1, emb2)\n",
    "# print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# batch1Aql = 'klasse 1'\n",
    "# batch2Aql = 'klasse 4, rejected'\n",
    "# batch1Healthy = '80'\n",
    "# batch1Rot = 0\n",
    "# batch1Scab = 0\n",
    "# batch1Blotch = 0\n",
    "# batch2Healthy = '68'\n",
    "# batch2Rot = '2'\n",
    "# batch2Scab = '5'\n",
    "# batch2Blotch = '5'\n",
    "# sentences = [f'De AQL van batch 1 is {batch1Aql} ',\n",
    "#           f'De AQL van batch 2 is {batch2Aql} ',\n",
    "#           f'Het aantal gezonde appels in batch 1 is {batch1Healthy}',\n",
    "#           f'Het aantal blotched appels in batch 1 is {batch1Blotch}',\n",
    "#           f'Het aantal rot appels in batch 1 is {batch1Rot}',\n",
    "#           f'Het aantal scabbed appels in batch 1 is {batch1Scab}',\n",
    "#           f'Het aantal blotched appels in batch 2 is {batch2Blotch}',\n",
    "#           f'Het aantal gezonde appels in batch 2 is {batch2Healthy}',\n",
    "#           f'Het aantal rot appels in batch 2 is {batch2Rot}',\n",
    "#           f'Het aantal scabbed appels in batch 2 is {batch2Scab}'\n",
    "#           ]\n",
    "\n",
    "# #Encode all sentences\n",
    "# embeddings = model.encode(sentences)\n",
    "\n",
    "# #Compute cosine similarity between all pairs\n",
    "# cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# #Add all pairs to a list with their cosine similarity score\n",
    "# all_sentence_combinations = []\n",
    "# for i in range(len(cos_sim)-1):\n",
    "#     for j in range(i+1, len(cos_sim)):\n",
    "#         all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "# #Sort list by the highest cosine similarity score\n",
    "# all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# print(\"Top-5 most similar pairs:\")\n",
    "# for score, i, j in all_sentence_combinations[0:5]:\n",
    "#     print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dus via input sentences appenden en dan de overeenkomende zin printen...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deVraag = model.encode(\"Hoeveel gezonde appels zitten er in batch 1?\")\n",
    "\n",
    "\n",
    "## cos_sim = util.cos_sim(deVraag, embeddings)\n",
    "# print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cos_sim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m all_sentence_combinations \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(cos_sim)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(cos_sim)):\n\u001b[0;32m      4\u001b[0m         all_sentence_combinations\u001b[39m.\u001b[39mappend([cos_sim[i][j], i, j])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cos_sim' is not defined"
     ]
    }
   ],
   "source": [
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# batch1Aql = 'klasse 1'\n",
    "# batch2Aql = 'klasse 4, rejected'\n",
    "# batch1Healthy = '80'\n",
    "# batch1Rot = 0\n",
    "# batch1Scab = 0\n",
    "# batch1Blotch = 0\n",
    "# batch2Healthy = '68'\n",
    "# batch2Rot = '2'\n",
    "# batch2Scab = '5'\n",
    "# batch2Blotch = '5'\n",
    "# # model = SentenceTransformer('paraphrase-MiniLM-L12-v2') bert-base-uncased\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# # Single list of sentences\n",
    "# sentences = [f'De AQL klasse van batch 1 is {batch1Aql} ',\n",
    "#           f'Het aantal gezonde appels in batch 1 is {batch1Healthy}',\n",
    "#           f'Het aantal blotched appels in batch 1 is {batch1Blotch}',\n",
    "#           f'Het aantal rotte appels in batch 1 is {batch1Rot}',\n",
    "#           f'Het aantal scabbed appels in batch 1 is {batch1Scab}',\n",
    "#           f'De AQL klasse van batch 2 is {batch2Aql} ',\n",
    "#           f'Het aantal blotched appels in batch 2 is {batch2Blotch}',\n",
    "#           f'Het aantal gezonde appels in batch 2 is {batch2Healthy}',\n",
    "#           f'Het aantal rotte appels in batch 2 is {batch2Rot}',\n",
    "#           f'Het aantal scabbed appels in batch 2 is {batch2Scab}'\n",
    "#           ]\n",
    "\n",
    "# #Compute embeddings\n",
    "# embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# #Compute cosine-similarities for each sentence with each other sentence\n",
    "# cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# #Find the pairs with the highest cosine similarity scores\n",
    "# pairs = []\n",
    "# for i in range(len(cosine_scores)-1):\n",
    "#     for j in range(i+1, len(cosine_scores)):\n",
    "#         pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "# #Sort scores in decreasing order\n",
    "# pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# for pair in pairs[0:10]:\n",
    "#     i, j = pair['index']\n",
    "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQL batch 1 : klasse 1  \t\t AQL batch 2 : klasse 4  \t\t Score: 0.9758\n",
      "rot batch 1 : 0 \t\t rot batch 2 : 2 \t\t Score: 0.9309\n",
      "blotch batch 1 : 0 \t\t blotch batch 2 : 5 \t\t Score: 0.9308\n",
      "scab batch 1 : 0 \t\t scab batch 2 : 5 \t\t Score: 0.9299\n",
      "gezond batch 1 : 80 \t\t gezond batch 2 : 68 \t\t Score: 0.9126\n",
      "blotch batch 1 : 0 \t\t scab batch 1 : 0 \t\t Score: 0.6123\n",
      "gezond batch 1 : 80 \t\t blotch batch 2 : 5 \t\t Score: 0.5960\n",
      "gezond batch 1 : 80 \t\t blotch batch 1 : 0 \t\t Score: 0.5960\n",
      "blotch batch 2 : 5 \t\t scab batch 2 : 5 \t\t Score: 0.5958\n",
      "blotch batch 2 : 5 \t\t gezond batch 2 : 68 \t\t Score: 0.5804\n"
     ]
    }
   ],
   "source": [
    "# ok top het werkt, maar het model heeft nog in sommige gevallen moeite met batch 1 van batch 2 te scheiden, \n",
    "# vermoed dat het ligt aan de overbodige info, \n",
    "# ik ga dus de antwoorden refactoren, \n",
    "# met de intentie enkel de relevante info er in te zetten, \n",
    "# en in de print van het antwoord de overbodig gramatica om het enigzins op praten te laten lijken weer terug laat komen\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "batch1Aql = 'klasse 1'\n",
    "batch2Aql = 'klasse 4'\n",
    "batch1Healthy = '80'\n",
    "batch1Rot = '0'\n",
    "batch1Scab = '0'\n",
    "batch1Blotch = '0'\n",
    "batch2Healthy = '68'\n",
    "batch2Rot = '2'\n",
    "batch2Scab = '5'\n",
    "batch2Blotch = '5'\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L12-v2') bert-base-uncased\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Single list of sentences\n",
    "sentences = [f'AQL batch 1 : {batch1Aql} ',\n",
    "          f'gezond batch 1 : {batch1Healthy}',\n",
    "          f'blotch batch 1 : {batch1Blotch}',\n",
    "          f'rot batch 1 : {batch1Rot}',\n",
    "          f'scab batch 1 : {batch1Scab}',\n",
    "          f'AQL batch 2 : {batch2Aql} ',\n",
    "          f'blotch batch 2 : {batch2Blotch}',\n",
    "          f'gezond batch 2 : {batch2Healthy}',\n",
    "          f'rot batch 2 : {batch2Rot}',\n",
    "          f'scab batch 2 : {batch2Scab}'\n",
    "          ]\n",
    "\n",
    "#Compute embeddings\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities for each sentence with each other sentence\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(len(cosine_scores)-1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for pair in pairs[0:10]:\n",
    "    i, j = pair['index']\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.3625, 0.5035, 0.4650, 0.8354, 0.4823, 0.3628, 0.4857, 0.4931, 0.8799,\n",
      "         0.4854]], device='cuda:0')\n",
      "rot batch 2 : 2\n"
     ]
    }
   ],
   "source": [
    "# # nice hij werkt bijna goed, behalve als je gezonde of de AQL vraagt uit batch 2... geen idee waarom? maybe ligt 80 dichter bij 2 dan 68?\n",
    "\n",
    "inputVraag = input() 'wat is de AQL van batch 1'\n",
    "deVraag = model.encode(inputVraag, convert_to_tensor=True)\n",
    "\n",
    "antwoordArray = util.dot_score(deVraag, embeddings)\n",
    "print(\"Similarity:\", util.dot_score(deVraag, embeddings))\n",
    "\n",
    "antwoordLocatie = antwoordArray.argmax()\n",
    "\n",
    "# print(antwoordLocatie)\n",
    "# print(antwoordLocatie.item())\n",
    "# de derde zin heeft de hoogste score [0.5115, 0.3944, !!!0.7727!!!, 0.5030, 0.5311, 0.5206, 0.5077, 0.7034, 0.5249, 0.5007]\n",
    "print(sentences[antwoordLocatie.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingewikkeldere versie als de tijd het toe laat : \n",
    "# https://huggingface.co/tasks/question-answering\n",
    "# https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
